## 知识嵌入知识点
#### 知识图谱嵌入

==**定义**==：为了解决前面提到的知识图谱表示的挑战，在词向量的启发下，研究者考虑如何将知识图谱中的实体和关系映射到连续的向量空间，并包含一些语义层面的信息，可以使得在下游任务中更加方便地操作知识图谱，例如问答任务[9]、关系抽取[10]等。对于计算机来说，连续向量的表达可以蕴涵更多的语义，更容易被计算机理解和操作。把这种将知识图谱中包括实体和关系的内容映射到连续向量空间方法的研究领域称为知识图谱嵌入（Knowledge Graph Embedding）、知识图谱的向量表示、知识图谱的表示学习（Representation Learning）、知识表示学习。【引用《知识图谱：方法、实践与应用》】

类似于词向量，知识图谱嵌入也是通过机器学习的方法对模型进行学习，与独热编码、词袋模型的最大区别在于，知识图谱嵌入方法的训练需要基于监督学习。在训练的过程中，可以学习一定的语义层信息，词向量具有的空间平移性也简单地说明了这点。类似于词向量，经典的知识图谱嵌入模型TransE的设计思想就是，如果一个三元组（h, r, t）成立，那么它们需要符合h+r ≈ t关系，例如：

```
vec（Rome）+vec（is−capital−of）≈vec（Italy）
```

所以，在知识图谱嵌入的学习过程中，不同的模型从不同的角度把相应的语义信息嵌入知识图谱的向量表示中。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E7%9F%A5%E8%AF%86%E5%B5%8C%E5%85%A5.png)

==**优点**==：

**使用向量的表达方式可以提高应用时的计算效率**，当把知识图谱的内容映射到向量空间时，相应的算法可以使用数值计算，所以计算的效率也会同时提高。
增加了下游应用设计的多样性。用向量表示后，知识图谱将更加适用于当前流行的机器学习算法，例如神经网络等方法。因为下游应用输入的并不再是符号，所以可以考虑的方法也不会仅局限于图算法。

将知识图谱嵌入作为下游应用的预训练向量输入，使得输入的信息不再是孤立的不包含语义信息的符号，而是已经经过一次训练，并且包含一定信息的向量。

如上所述，**知识图谱的嵌入方法可以提高计算的效率，增加下游应用的多样性，并可以作为预训练，为下游模型提供语义支持**，所以对其展开的研究具有很大的应用价值和前景。

==**主要方法**==：

 **转移距离模型（Translational Distance Model**的主要思想是将衡量向量化后的知识图谱中三元组的合理性问题，转化成衡量头实体和尾实体的距离问题。这一方法的重点是如何设计得分函数，得分函数常常被设计成利用关系把头实体转移到尾实体的合理性的函数。
受词向量的启发，由词与词在向量空间的语义层面关系，可以拓展到知识图谱中头实体和尾实体在向量空间的关系。也就是说，同样可以考虑把知识图谱中的头实体和尾实体映射到向量空间中，且它们之间的联系也可以考虑
成三元组中的关系。TransE[12]便是受到了词向量中平移不变性的启发，在 TransE 中，把实体和关系都表示为向量，对于某一个具体的关系（head, relation, tail），把关系的向量表示解释成头实体的向量到尾实体的向量的转移向量（Translation vector）。也就是说，如果在一个知识图谱中，某一个三元组成立，则它的实体和关系需要满足关系head+relation≈tail。

 **语义匹配模型**：相比于转移距离模型，语义匹配模型（Semantic Matching Models），更注重挖掘向量化后的实体和关系的潜在语义。该方向的模型主要是RESCAL[13]以及它的延伸模型。

- **RESCAL模型**的核心思想是将整个知识图谱编码为一个三维张量，由这个张量分解出一个核心张量和一个因子矩阵，核心张量中每个二维矩阵切片代表一种关系，因子矩阵中每一行代表一个实体。

- **DistMul**：DistMul通过限制Mr为对角矩阵简化 RESCAL 模型，也就是说其限制Mr=diag（r）。

- **ComplEx模型**：考虑到复数的乘法不满足交换律，所以在该模型中实体和关系的向量表示不再依赖实数而是放在了复数域，从而其得分函数不具有对称性。也就是说，对于非对称的关系，将三元组中的头实体和尾实体调换位置后可以得到不同的分数。

**考虑附加信息的模型**：
除了仅仅依靠知识库中的三元组构造知识图谱嵌入的模型，还有一些模型考虑额外的附加信息进行提升。

实体类型是一种容易考虑的额外信息。在知识库中，一般会给每个实体设定一定的类别，例如Rome具有city的属性、Italy具有country的属性。最简单的考虑实体类型的方法是在知识图谱中设立类似于IsA这样的可以表示实体属性的关系，例如

```
（Rome,IsA,city）
（Italy,IsA,Country）
```

这样的三元组。当训练知识图谱嵌入的时候，考虑这样的三元组就可以将属性信息考虑到向量表示中。也有一些方法考虑相同类型的实体需要在向量表示上更加接近。

关系路径也可以称为实体之间的多跳关系（Multi-hop Relationships），一般就是指可以连接两个实体的关系链，例如

```
（Rome,is−capital−of,Italy）
（Italy,is−country−of,Europe）.
```

从 Rome 到 Europe 的关系路径就是一条is−capital−of→is−country−of关系链。当前很多方法也尝试考虑关系路径来提升嵌入模型，这里的关键问题是考虑如何用相同的向量表达方式来表达路径。在基于路径的 TransE，也就是 PTransE[17]中，考虑了相加、相乘和RNN三种用关系表达关系路径的方法：

```
p=r1+r2+⋯+rl
p=r1∙r2∙⋯∙rl
ci=f（W[ci−1;ri]）.
```

在基于 RNN 的方法中，令c1=r1并且一直遍历路径中的关系，直到最终p=cn。对于某一个知识库中存在的三元组，其两个实体间的关系路径p需要和原本两个实体间关系的向量表示相接近。

**文本描述（Textual Descriptions**指的是在一些知识图谱中，对实体有一些简要的文本描述，如图2-24所示，这些描述本身具有一定的语义信息，对提高嵌入的质量有一定的提升。除了某些知识库本身具有的文本描述，也可以使用外部的文本信息和语料库。Wang[18]提出了一种在知识图谱嵌入的过程中使用文本信息的联合模型，该模型分三个部分：知识模型、文本模型和对齐模型。其中，知识模型对知识图谱中的实体和关系做嵌入，这是一个 TransE的变种；文本模型对语料库中词语进行向量化，这是一个 Skip-gram模型的变种；对齐模型用来保证知识图谱中的实体和关系与单词的嵌入在同一个空间中。联合模型在训练时降低来自三个子模型的损失之和。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E4%BF%A1%E6%81%AF.png)

逻辑规则（Logical Rules）也是常被用来考虑的附加信息，这里讨论的重点主要是霍恩子句，例如简单规则

```
∀x,y:IsDirectorOf（x,y）⇒BeDirectedBy（y,x）
```

说明了两个不同的关系之间的关系。Guo[19]提出了一种以规则为指导的知识图谱嵌入方法，其中提出的软规则（Soft rule）指的是使用AMIE+规则学习方法在知识图谱中挖掘的带有置信度的规则，该方法的整体框架是一个迭代的过程，其中包含两个部分，称为软标签预测阶段（Soft Label Prediction）和嵌入。

修正阶段（Embedding Rectification）。简单来说，就是讲规则学习和知识图谱嵌入学习互相迭代，最后使得知识图谱嵌入可以融入一定的规则信息。

==**应用**==：

1. **链接预测**：
链接预测（Link Prediction）指通过一个已知的实体和关系预测另一个实体，或者通过两个实体预测关系。简单来说，也就是（h,r,?）,（?,r,t）,（h,?,t）三种知识图谱的补全任务，被称为链接预测。

当知识图谱的嵌入被学习完成后，知识图谱嵌入就可以通过排序完成。例如需要链接预测(Roma, is-capital-of, ?)，可以将知识图谱中的每个实体都放在尾实体的位置上，并且放入相应的知识图谱嵌入模型的得分函数中，计算不同实体作为该三元组的尾实体的得分，也就是该三元组的合理性，得分最高的实体会被作为链接预测的结果。
链接预测也常被用于评测知识图谱嵌入。一般来说，会用链接预测的正确答案的排序评估某种嵌入模型在链接预测上的能力，**比较常见的参数有平均等级（Mean Rank）、平均倒数等级（Mean Reciprocal Rank）和命中前n（Hist@n）**。

2. **三元组分类**：三元组分类（Triple Classification）指的是给定一个完整的三元组，判断三元组的真假。这对于训练过的知识图谱向量来说非常简单，只需要把三元组各个部分的向量表达带入相应的知识图谱嵌入的得分函数，三元组的得分越高，其合理性和真实性越高。

3. **实体对齐**： 
实体对齐（Entity Resolution）也称为实体解析，任务是验证两个实体是否指代或者引用的是同一个事物或对象。该任务可以删除同一个知识库中冗余的实体，也可以在知识库融合的时候从异构的数据源中找到相同的实体。一种方法是，如果需要确定 x、y 两个实体指代同一个对象有多大可能，则使用知识图谱嵌入的得分函数对三元组(x, EqualTo, y)打分，但这种方法的前提是需要在知识库中存在 EqualTo 关系。也有研究者提出完全根据实体的向量表示判断，例如设计一些实体之间的相似度函数来判断两个实体的相似程度，再进行对齐。

4.  **问答系统**：利用知识图谱完成问答系统是该任务的一个研究方向，该任务的重心是对某一个具体的通过自然语言表达的问题，使用知识图谱中的三元组对其进行回答，如下：


```
A: Where is the capital of Italy？
Q: Rome（Rome, is-capital-of, Italy）
A: Who is the president of USA？
Q: Donald Trump（Donald Trump, is-president-of, USA）
```

文献[9]介绍了一种借助知识图谱嵌入完成该问题的方法。简单来说就是设计一种得分函数，使问题的向量表示和其正确答案的向量表示得分较高。S（q,a）是被设计出来的得分函数
S（q,a）=（Wφ（q））⊺（Wψ（a））.
式中，W为包含词语、实体和关系的向量表示的矩阵；φ（q）为词语出现的稀疏向量；ψ（a）为实体和关系出现的稀疏向量。简单来说，Wφ（q）和Wψ（a）可以分别表示问题和答案的向量表示。当a是q的正确答案时，得分函数S（q,a）被期望得到一个较高的分数，反之亦然。

5. **推荐系统**：推荐系统的本质是对用户推荐其没有接触过的、但有可能会感兴趣或者购买的服务或产品，包括电影、书籍、音乐、商品等。协同过滤算法（Collaborative Filtering）对用户和物品项目之间的交互进行建模并作为潜在表示取得了很好的效果。
在知识图谱嵌入的发展下，推荐系统也尝试借助知识图谱的信息提高推荐系统的能力。例如，Zhang[20]尝试知识图谱中的三元组、文本信息和图像信息对物品项目进行包含一定语义的编码得到相应的向量表示，然后使用协同过滤算法对用户进行向量表示，对两个向量表示相乘得到分数，得分越高说明该用户越喜好该商品。

#
#### 独热编码
- [x] **学习sklearn和kagggle时遇到的问题，什么是独热编码？为什么要用独热编码？什么情况下可以用独热编码？以及和其他几种编码方式的区别。**

首先了解机器学习中的特征类别：连续型特征和离散型特征

拿到获取的原始特征，必须对每一特征分别进行归一化，比如，特征A的取值范围是[-1000,1000]，特征B的取值范围是[-1,1].如果使用logistic回归，w1* x1+w2 *x2，因为x1的取值太大了，所以x2基本起不了作用。所以，必须进行特征的归一化，每个特征都单独进行归一化。

**对于连续性特征：**

- Rescale bounded continuous features: All continuous input that are bounded, rescale them to [-1, 1] through x = (2x - max - min)/(max - min). 线性放缩到[-1,1]
- Standardize all continuous features: All continuous input should be standardized and by this I mean, for every continuous feature, compute its mean (u) and standard deviation (s) and do x = (x - u)/s. 放缩到均值为0，方差为1

**对于离散性特征：**

Binarize categorical/discrete features: 对于离散的特征基本就是按照one-hot（独热）编码，该离散特征有多少取值，就用多少维来表示该特征。

#
==**一. 什么是独热编码？**==

**定义**：独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。

**举例如下：**

假如有三种颜色特征：红、黄、蓝。 在利用机器学习的算法时一般需要进行向量化或者数字化。那么你可能想令 红=1，黄=2，蓝=3. 那么这样其实实现了标签编码，即给不同类别以标签。然而这意味着机器可能会学习到“红<黄<蓝”，但这并不是我们的让机器学习的本意，只是想让机器区分它们，并无大小比较之意。所以这时标签编码是不够的，需要进一步转换。因为有三种颜色状态，所以就有3个比特。即红色：1 0 0 ，黄色: 0 1 0，蓝色：0 0 1 。如此一来每两个向量之间的距离都是根号2，在向量空间距离都相等，所以这样不会出现偏序性，基本不会影响基于向量空间度量算法的效果。

自然状态码为：000,001,010,011,100,101

独热编码为：000001,000010,000100,001000,010000,100000

**独热模型图解：**

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E7%8B%AC%E7%83%AD%E6%A8%A1%E5%9E%8B.png)

**来一个sklearn的例子：**

```
from sklearn import preprocessing
enc = preprocessing.OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])    # fit来学习编码
enc.transform([[0, 1, 3]]).toarray()    # 进行编码
```
**输出**：array([[ 1., 0., 0., 1., 0., 0., 0., 0., 1.]])

数据矩阵是4*3，即4个数据，3个特征维度。

0 0 3 观察左边的数据矩阵，第一列为第一个特征维度，有两种取值0\1. 所以对应编码方式为10 、01

1 1 0 同理，第二列为第二个特征维度，有三种取值0\1\2，所以对应编码方式为100、010、001

0 2 1 同理，第三列为第三个特征维度，有四中取值0\1\2\3，所以对应编码方式为1000、0100、0010、0001



再来看要进行编码的参数[0 , 1, 3]， 0作为第一个特征编码为10, 1作为第二个特征编码为010， 3作为第三个特征编码为0001. 故此编码结果为 1 0 0 1 0 0 0 0 1

#

==**二. 为什么要独热编码？**==

正如上文所言，独热编码（哑变量 dummy variable）是因为大部分算法是基于向量空间中的度量来进行计算的，为了使非偏序关系的变量取值不具有偏序性，并且到圆点是等距的。使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1。

**为什么特征向量要映射到欧式空间？**

将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。

#

==**三 .独热编码优缺点**==
- **优点**：独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。

- **缺点**：

**较为笼统的说法**： 当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。

**较为详细的说法**：

**背景**：传统的NLP方法，如HMM（隐马尔科夫模型）、CRF（条件随机场）、最大熵模型（MEM）等概率图模型都把文本看做一个有限状态机进行研究，因而无须对词汇进行量化处理。
随着深度神经网络的广泛应用，越来越多的研究者将关注点转向NLP与深度学习方法的结合。而面临的第一个问题就是如何将自然语言中使用的一系列字符转化为神经网络输入层所需的向量。

one-hot表示法提出了一种简单编码的思路，对于一个大小为N的词典（包含了自然语言中所需要的字符和词汇），其中的每一个字符映射为一个N×1的稀疏向量v，假设该字符在词表中的位置为i，则v的第i个元素为1，其余元素为0。

**one-hot表示法面临以下两个问题：**

1. **词表中的每个词向量相互正交**。

然而在实际的语言使用中，词汇之间是有语义相似性的（例如：good, excellent），one-hot表示法无法体现这样的语义关系；

2. **内存开销庞大，计算复杂度高**。需要说明的是，如果采用稀疏矩阵的表示方法（常用的有三元组法表示）词向量的存储并不会占据过多内存，而建立训练用的神经网络需要存储过多的训练参数，这个开销较大，另外在进行卷积层运算和优化算法时时间复杂度和空间复杂度较高。


从上面两个致命缺陷来看，似乎one-hot表示法一无是处，是否应该摒弃它并重新探索出一种新的表示法呢？基于one-hot模型做一些改进也许更为有效。这样的改进需要完成两个目的，一是使得新的词向量具备一定的语义相似性，二是减小内存存储。语义相似性在数学上并没有直观的定义，但我们可以假设两个词向量之间的线性相关性越高，相似性越强，减小内存的最直接思路就是减小词向量的维度。于是接下来的任务就明确起来，

1. 利用现存的文本信息，赋予one-hot向量语义信息；
1. 将one-hot向量投影到更低维度的空间；
CBOW(连续词袋模型)和Ship-Gram模型分别以两种相对的思路解决了上述两个问题。


#
==**四. 什么情况下(不)用独热编码？**==

**用**：独热编码用来解决类别型数据的离散值问题。

**不用**：将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。 有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。 Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。

**总的来说**，要是one hot encoding的类别数目不太多，建议优先考虑。
#

==**五. 什么情况下(不)需要归一化？**==

**需要**： 基于参数的模型或基于距离的模型，都是要进行特征的归一化。

**不需要**：基于树的方法是不需要进行特征的归一化，例如随机森林，bagging 和 boosting等

#
==**六. 标签编码LabelEncoder**==

**作用**： 利用LabelEncoder() 将转换成连续的数值型变量。即是对不连续的数字或者文本进行编号例如：


```
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit([1,5,67,100])
le.transform([1,1,100,67,5])
```
输出： array([0,0,3,2,1])


```
>>> le = preprocessing.LabelEncoder()
>>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
LabelEncoder()
>>> list(le.classes_)
['amsterdam', 'paris', 'tokyo']     # 三个类别分别为0 1 2
>>> le.transform(["tokyo", "tokyo", "paris"]) 
array([2, 2, 1]...)    
>>> list(le.inverse_transform([2, 2, 1]))   # 逆过程
['tokyo', 'tokyo', 'paris']
```
**限制**：上文颜色的例子已经提到标签编码了。Label encoding在某些情况下很有用，但是场景限制很多。再举一例：比如有[dog,cat,dog,mouse,cat]，我们把其转换为[1,2,1,3,2]。这里就产生了一个奇怪的现象：dog和mouse的平均值是cat。所以目前还没有发现标签编码的广泛使用。

#
**基本的机器学习过程**

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B.jpg)

#
#### 词袋模型

==**定义**==：词袋模型（Bag-of-Words,BoW）是一种对文本中词的表示方法，也就是说词袋模型能够把一个句子转化为向量表示，是比较简单直白的一种方法，它不考虑句子中单词的顺序，只考虑词表（vocabulary）中单词在这个句子中的出现次数。该方法将文本想象成一个装词的袋子，不考虑词之间的上下文关系，不关心词在袋子中存放的顺序，仅记录每个词在该文本（词袋）中出现的次数。具体的方法是先收集所有文本的可见词汇并组成一个词典，再对所有词进行编号，对于每个文本，可以使用一个表示每个词出现次数的向量来表示，该向量的每一个维度的数字表示该维度所指代的词在该文本中出现的次数。

**词袋模型图解**:

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B.png)

在文本doc_1中，Rome出现32次，Paris出现14次，France出现0次。

EG：


```
"John likes to watch movies, Mary likes movies too"

"John also likes to watch football games"
```

对于这两个句子，我们要用词袋模型把它转化为向量表示，这两个句子形成的词表（不去停用词）为：


```
[‘also’, ‘football’, ‘games’, ‘john’, ‘likes’, ‘mary’, ‘movies’, ‘to’, ‘too’, ‘watch’]
```
因此，它们的向量表示为：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E8%AF%8D%E8%A2%8B.jpg)


scikit-learn中的CountVectorizer()函数实现了BOW模型，下面来看看用法：


```
from sklearn.feature_extraction.text import CountVectorizer
corpus = [
    "John likes to watch movies, Mary likes movies too",
    "John also likes to watch football games",
]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus) //学习
print(vectorizer.get_feature_names())//得到特证名
print(X.toarray())//得到数组

#输出结果：
#['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']
#[[0 0 0 1 2 1 2 1 1 1]
# [1 1 1 1 1 0 0 1 0 1]]
```
#
#### 词向量
==**定义**==：上面对词的表示方法并没有考虑语义层面的信息，为了更多地表示词与词之间的语义相似程度，提出词的分布式表示，也就是基于上下文的稠密向量表示法，通常称为词向量或词嵌入（Word Embedding）。产生词向量的手段主要有三种：

- Count-based。基于计数的方法，简单说就是记录文本中词的出现次数。

- Predictive。基于预测的方法，既可以通过上下文预测中心词，也可以通过中心词预测上下文。

- Task-based。基于任务的，也就是通过任务驱动的方法。通过对词向量在具体任务上的表现效果对词向量进行学习。

#

#### 开源工具word2vec
向量的表示中如果蕴含了上下文信息，那么将会更加接近自然语言的本质；并且，由于相似的词有相似的表示方法，甚至可以进行一些运算，例如：人类-男人=女人。但是，上述讨论中，有一个很大的缺陷，那就是词的向量表示维度过大，一个词要用大量其余的词来表示，为后续运算带来了很大的麻烦。因此，我们需要找到一种更好的表示方法，这种方法需要满足如下两点要求：

- 携带上下文信息
- 词的表示是稠密的

事实证明，通过神经网络来进行建模，可以满足这两点要求，这就是Word2Vec的思想。Word2Vec建模方法又有两种：CBOW和Skip–Gram。

**Word2Vec**

Word2vec使用单个隐藏层，完全连接的神经网络如下所示。隐藏层中的神经元都是线性神经元。输入层设置为具有与用于训练的词汇中的单词一样多的神经元。隐藏图层大小设置为生成的单词向量的维度。输出图层的大小与输入图层相同。

因此，假设用于学习单词向量的词汇表由V个单词组成并且N为单词向量的维度(每个单词拥有N个特征)，则对隐藏层连接的输入可以由大小为VxN的矩阵WI表示，其中每行表示词汇单词。以相同的方式，可以通过矩阵WO来描述从隐藏层到输出层的连接大小为NxV。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/word2vec.jpg)

**EG：**

具有以下句子的训练语料库：

“The dog saw a cat”，“Dog chasing cat”，“Cat climbed a tree”

语料库词汇有八个单词

['a','cat','chasing','climbed','dog','saw','the','tree']。按字母顺序排序后，每个单词都可以通过其索引引用。对于这个例子，我们的神经网络将有八个输入神经元和八个输出神经元。让我们假设我们决定在隐藏层中使用三个神经元。这意味着WI和WO将分别是8×3和3×8矩阵。在训练开始之前，这些矩阵被初始化为随机值，假设WI和WO初始化为以下值：

WI =

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/WI.jpg)

WO =

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/WO.png)

假设我们希望网络学习单词“cat”和“climbed”之间的关系。也就是说，当“cat”输入到网络时，网络应该显示“climbed”的高概率。在单词嵌入术语中，单词“cat”被称为context word，单词“climbed”被称为target word。在这种情况下，输入矢量X将是[0 1 0 0 0 0 0 0]t，目标矢量是[0 0 0 1 0 0 0 0]t。

利用表示“cat”的输入向量，可以将隐藏层神经元的输出计算为：

H = X*WI = [-0.490796 -0.229903 0.065460]

实际上隐藏神经元输出的向量H复制了WI矩阵的第二行的权重，因为输入层是One-hot编码，cat又是第二个word，所以会复制第二行的数值。对隐藏到输出层执行类似的操作，输出层神经元的激活矢量可以写成：

H_ = H*WO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686 0.112928]

由于目标是为输出层中的单词生成概率，因此在输出层后面，我们会采用前文所述的Softmax。

#
#### softmax回归模型

==**作用：**==

通俗理解，softmax用作归一化。

细讲：在机器学习尤其是深度学习中，softmax是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。

Softmax将多个神经元的输出，映射到（0,1）区间内，可以看成是当前输出是属于各个分类的概率，从而来进行多分类。



**EG：**

一个多分类问题，C = 4。线性分类器模型最后输出层包含了四个输出值，分别是：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/1.png)

经过Softmax处理后，数值转化为相对概率：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/2.png)

一般用在多分类的模型中，在神经网络、深度学习中紧跟着输出层；

==**为什么叫Softmax?**==

++解释1++：假设a>b，取max，那么b没有任何可能性。但是如果给两者按照大小赋予概率值，那么b依旧有被取到的可能性，所以取名Softmax;

++解释2++：首先我们简单来看看softmax是什么意思。顾名思义，softmax由两个单词组成，其中一个是max。对于max我们都很熟悉，比如有两个变量a,b。如果a>b，则max为a，反之为b。用伪码简单描述一下就是 if a > b return a; else b。
另外一个单词为soft。max存在的一个问题是什么呢？如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。

==**softmax如何计算：**==


假设我们有一个数组V，Vi表示V中的第i个元素，那么这个元素的Softmax值就是：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/3.png)

也就是说，是该元素的指数值，与所有元素指数值和的比值;

实际应用中，使用 Softmax 需要注意数值溢出的问题。因为有指数运算，如果 V 数值很大，经过指数运算后的数值往往可能有溢出的可能。所以，需要对 V 进行一些数值处理：即 V 中的每个元素减去 V 中的最大值：


![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/4.png)

上面公式对应的py代码：


```py
scores = np.array([123, 456, 789])    # example with 3 classes and each having large scores
scores -= np.max(scores)    # scores becomes [-666, -333, 0]
p = np.exp(scores) / np.sum(np.exp(scores))

```

```py
# -*- coding: utf-8 -*-
import math
 
V = [9,6,3,1]
 
v1 = math.exp(9)
v2 = math.exp(6)
v3 = math.exp(3)
v4 = math.exp(1)
 
v_sum = v1 + v2 + v3 + v4
 
print(v1/v_sum, v2/v_sum, v3/v_sum, v4/v_sum)
```



#
**开源工具word2vec包含的CBoW模型和Skip-gram模型**

两者的功能：它们的基本功能是将高维度的one-hot词向量嵌入到低维并赋予其语义相似性。

代码实现：利用PyTorch的自动梯度模块求解CBOW模型进行。



#### 连续词袋模型（Continues-Bag-of-Words，CBoW）

==**定义**==：和之前提到的BoW相似之处在于该模型也不用考虑词序的信息。其主要思想是，用上下文预测中心词，从而训练出的词向量包含了一定的上下文信息。如图所示，其中wn是中心词， wn−2,wn−1,wn+1,wn+2为该中心词的上下文的词。将上下文词的独热表示与词向量矩阵E相乘，提取相应的词向量并求和得到投影层，然后再经过一个 Softmax层最终得到输出，输出的每一维表达的就是词表中每个词作为该上下文的中心词的概率。整个模型在训练的过程就像是一个窗口在训练语料上进行滑动，所以被称为连续词袋模型。

==**流程**：==

1.**语义的提取**

语义信息可以通过两种方式获得，一种是规则，例如单词"a"后面一般接名词；另一种是经验，所谓经验，即从大量的自由文本中获得，神经网络是一种基于经验的方法。那么如何在大量的文本中定义语义信息的形式呢？语言这种信息通常以字符的时间序列表示。这类似于语音或者音乐（音符的时间序列）的传递方式。语义的产生是由于字符本身的指代以及字符的排列组合。我们已有的词表编码了每个字符，而字符的组合则需要从大量的文本中获取。有了上面的理解，语义的相似性就可以理解成字符所在序列的相似性。例如下面两句话


```
I like watching films.
I enjoy reading.
```
上面两句话中的like和enjoy用的语境相似。也就是说，表达某一个词的语义，可以用它附近的词（one-hot向量）进行表示，拿上面的举例，(I, watching, films)就某种意义上说明了like的语义。

这里，我有一个==疑问==，就是如何辨别反义词的语义?例如"I hate reading"中的hate与enjoy的语境相同，但是却有着截然相反的含义

无论如何，利用这样的思路可以从文本中获取大量的语义信息，并且我们可以自由的选择语义窗的大小。




![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/cbow.PNG)


需要用到的开源模块：
```
numpy——基础的数据类型
torch.autograd——用于梯度下降法求解
sklearn.decomposition——用于PCA主成分分析降维
matplotlib.pyplot——简单可视化工具
nltk——词频统计工具

```
用到的数据集：手头上现存的CoNLL-2003训练集。




#

#### Skip-Gram模型

**定义**：Skip-Gram模型可以看做是CBOW模型的镜像，如果说CBOW是让计算机做缺词填空，那么Skip-Gram就类似于让计算机根据某一个词去想象它可能的语境，这样考虑，按我的理解，似乎Skip-Gram模型更加赋予挑战性，也更可能不精确。

#


