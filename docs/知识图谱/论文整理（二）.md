### 第一篇
ACL 2020 | A Relational Memory-based Embedding Model for Triple Classification and Search Personalization 用于三元组分类和搜索个性化的基于关系记忆的嵌入模型

（作者：Dai Quoc Nguyen, Tu Dinh Nguyen, Dinh Phung）

#### 问题
作者认为，现有的知识图谱嵌入方法通常在记忆有效三元组方面有局限。作者认为，现有方法不能有效捕捉实体和关系之间的潜在依赖。从 Trans 系列的发展来看，这不是一个新问题，但作者有了新的思路。

#### 方案
##### 思想
序列模型对三元组进行建模，用于三元组分类和搜索个性化任务。

作者将其嵌入模型称为 R-MeN，它使用一个关系记忆网络来编码三元组，推断新的三元组。具体而言，R-MeN 将每个三元组与额外的位置嵌入（positional embedding）一起转换成 3 个输入向量序列。然后，R-MeN 使用 transformer 的自注意力（self-attention）机制来引导记忆与每个输入向量进行交互，以产生一个编码向量。最终，R-MeN 将这 3 个编码向量反馈给基于 CNN 的解码器，返回一个三元组的得分。

R-MeN 基于 transformer 的自注意力机制，以 CNN 为解码器的记忆交互的形式对知识图谱学习嵌入。
##### R-MeN模型
- 位置嵌入
- 记忆
- 卷积：对于已编码的向量，作者使用基于 CNN 的解码器来计算三元组分数。
- 损失函数：训练使用 Adam optimizer，并使用了负采样。


#### 实验
##### 数据集
作者在两个任务上使用了三个数据集：
- 三元组分类：WN11 和 FB13
- 搜索个性化：SEARCH17
对于 SEARCH17 的实验，模型被训练为三元组 (query，user,document) 计算分数。
##### 消融实验
在消融实验部分，作者尝试去除位置嵌入与不使用关系记忆网络。可见，R-MeN 的主要表现提升体现在关系记忆网络上，位置嵌入发挥了次要的作用。

#### ==创新点==
作者主要面向的应用场景是搜索个性化和三元组分类：三元组分类旨在预测给定的三元组是否有效；搜索个性化旨在对面向用户的搜索引擎返回的相关文档进行重新排序。

主要贡献在于假设三元组的三个元素的相对位置信息对于三元组分类问题是有效的，并且通过序列建模取得了好的效果。

#
### 第二篇
ACL 2020 | A Re-evaluation of Knowledge Graph Completion Methods 知识图谱补全方法的重新评估

（作者：Zhiqing Sun，Shikhar Vashishth，Soumya Sanyal，Partha Talukdar，Yiming Yang）

#### 文章类型
实证类的研究，回顾了现有的知识图谱嵌入方法，并从独特的角度提出了质疑，对研究者在今后的研究具有一定启发意义

#### 知识图谱补全简介
真实世界的知识图谱通常是不完整的，启发研究者探索自动补全知识图谱的方法，一种常见方法是将知识图谱的实体和关系嵌入连续向量或矩阵空间中，并使用设计好的评分函数度量一个三元组的可能性。

##### 方法
基于平移距离的 (translation distance) 

语义匹配 (semantic matching) 

一部分基于神经网络的方法也被提出，如使用 CNN [2-3]、RNN[4-5]、GNN[6-7]、Capsule Network[8] 

#### 观察
作者展示了观察到的一些事实：
- 从模型表现方面，作者观察到不同数据集上模型的提升幅度不一致；
- 从神经网络方面，作者观察到有效与无效三元组的得分分布不正常。

#### 基准测试数据集上不一致的提升
一些模型在 FB15k-237 和 WN18RR 两个数据集上的 MRR 提升并不一致。例如 ConvKB、CapsE、KBAT 等方法在两个数据集上的提升差距较大。
![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E6%95%B0%E6%8D%AE.jpg)

#### 对评分函数的观察
三元组评分函数的不合理主要体现在：有效三元组和无效三元组分数的分布差异。

对于归一化的分数，58.5%的无效三元组和有效三元组拥有一致的分数。
![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E8%A7%82%E5%AF%9F.jpg)

==？==

作者对不同方法使用的 ReLU 激活函数进行深入研究，发现对于 FB15k-237 数据集，大量神经元在 ReLU 激活函数之后变为 0. 因此，一些三元组表示在前向传播中变得非常相似，导致了相同的分数。这是否说明，当前的方法完全没有足够的能力对三元组进行建模？从分数分布的角度看，是否体现出了典型的欠拟合？



![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/1.jpg)

**什么是激活函数？**

激活函数来源于activation function的翻译，那activation有活化，激活的含义，作用是将数据进行非线性处理，以至于能够提高通过这组数据下训练出的模型的拟合性。（将线性数据变成非线性的）

**为什么要用激活函数?**

激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。

如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。我们希望我们的神经网络不仅仅可以学习和计算线性函数，而且还要比这复杂得多。同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。

**为什么我们需要非线性函数？**
非线性函数是那些一级以上的函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。
而这一切都归结于这一点，我们需要应用激活函数f（x），以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。因此，使用非线性激活函数，我们便能够从输入输出之间生成非线性映射。

激活函数的另一个重要特征是：它应该是可以区分的。我们需要这样做，以便在网络中向后推进以计算相对于权重的误差（丢失）梯度时执行反向优化策略，然后相应地使用梯度下降或任何其他优化技术优化权重以减少误差


**为什么叫激活函数？**

当输入激励达到一定强度，神经元就会被激活，产生输出信号。模拟这一细胞激活过程的函数，就叫激活函数。

--

还有这类函数最典型的是sigmoid函数和ReLU函数，一个是类似s型曲线，是不是把数据分为0和1。

另外的ReLU函数跟sigmoid函数的区别是什么。


**线性整流函数（Rectified Linear Unit, ReLU）**，又称修正线性单元，是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。

相比于其它激活函数来说，ReLU有以下优势：对于线性函数而言，ReLU的表达能力更强，尤其体现在深度网络中；而对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此不存在梯度消失问题(Vanishing Gradient Problem)，使得模型的收敛速度维持在一个稳定状态。这里稍微描述一下什么是梯度消失问题：当梯度小于1时，预测值与真实值之间的误差每传播一层会衰减一次，如果在深层模型中使用sigmoid作为激活函数，这种现象尤为明显，将导致模型收敛停滞不前。

**过拟合和欠拟合：**

机器学习中一个重要的话题便是模型的泛化能力，泛化能力强的模型才是好模型。对于训练好的模型：


- 若在训练集表现差，不必说在测试集表现同样会很差，这可能是欠拟合导致；
- 若模型在训练集表现非常好，却在测试集上差强人意，则这便是过拟合导致的。

过拟合与欠拟合也可以用 Bias 与 Variance 的角度来解释，欠拟合会导致高 Bias ，过拟合会导致高 Variance ，所以模型需要在 Bias 与 Variance 之间做出一个权衡。

使用简单的模型去拟合复杂数据时，会导致模型很难拟合数据的真实分布，这时模型便欠拟合了，或者说有很大的 Bias，Bias 即为模型的期望输出与其真实输出之间的差异；有时为了得到比较精确的模型而过度拟合训练数据，或者模型复杂度过高时，可能连训练数据的噪音也拟合了，导致模型在训练集上效果非常好，但泛化性能却很差，这时模型便过拟合了，或者说有很大的 Variance，这时模型在不同训练集上得到的模型波动比较大，Variance 刻画了不同训练集得到的模型的输出与这些模型期望输出的差异。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/2.jpg)

模型处于过拟合还是欠拟合，可以通过画出误差趋势图来观察。

- 若模型在训练集与测试集上误差均很大，则说明模型的 Bias 很大，此时需要想办法处理 under-fitting ；
- 若是训练误差与测试误差之间有个很大的 Gap ，则说明模型的 Variance 很大，这时需要想办法处理 over-fitting。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/3.jpg)

一般在模型效果差的第一个想法是增多数据，其实增多数据并不一定会有更好的结果，因为欠拟合时增多数据往往导致效果更差，而过拟合时增多数据会导致 Gap 的减小，效果不会好太多，多以当模型效果很差时，应该检查模型是否处于欠拟合或者过拟合的状态，而不要一味的增多数据量。

**过拟合的概念**

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E8%BF%87%E6%8B%9F%E5%90%88.jpg)

所以过拟合有两种原因：

- 训练集和测试集特征分布不一致（白天鹅黑天鹅）
- 模型太过复杂（记住了每道题）

解决过拟合也从这两方面下手，收集多样化的样本，简化模型，交叉检验。

#### ==创新点==
##### 知识图谱补全的评估准则
作者认为，其中一些方法的评估准则 (evalution protocol) 并不合理，导致表现被夸大。作者尝试对知识图谱补全方法进行重新评估，其重新评估的重点在于对评估方法的审视。

作者提出了新的评估准则 (protocol)，并证明不合理的评估准则是基于神经网络的嵌入方法表现异常的关键原因。

作者提出评估准则的出发点基于他们的观察，即打破三元组在同一分数上的大量重复。作者没有尝试对不同补全方法的评分函数的原理分析，或者是从如何改进 ReLU 激活函数的表现进行分析，而是从评估准则和三元组分数之间的关系进行分析。

对于一个候选的三元组集合 $\mathcal{T'}$，如果模型中对多个三元组有同一分数，我们应该选择其中一个三元组。假设三元组是以固定方式排序的，作者给出了三个不同的准则：
- TOP：正确的三元组插入到 $\mathcal{T'}$ 的头部
- BOTTOM：正确的三元组插入到 $\mathcal{T'}$ 的尾部
- RANDOM：正确的三元组随机插入到 $\mathcal{T'}$ 中的一个位置

#### 实验
##### 数据集
使用 FB15k 的子集 FB15k-237，逆关系被删除防止测试时直接从训练三元组推断。

作者将分析的模型分为两类：未受影响的和受影响的，即在不同评估准则下表现一致的模型和表现不一致的模型。

评估的指标包括：MRR（mean reciprocal rank）、MR（mean rank）和 Hit@10（H@10）.

MRR：把标准答案在被评价系统给出结果中的排序取倒数作为它的准确度，再对所有的问题取平均。

作者表示，ConvE、RotatE 和 TuckER 的原论文使用的是 RANDOM，而 ConvKB、CapsE 和 KBAT 使用的是 TOP.

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/4.jpg)

图中是 TOP 和 BOTTOM 相比于 RANDOM 的比较。TOP 相比于 RANDOM 并不能严格评估模型，表现显著偏高。而 BOTTOM 也显示出它的不公平，正确的三元组排名较低。作者认为 RANDOM 是严格且公平的。

#### 结论
个人认为，NLP 社区应当鼓励质疑类的工作和无效探索的工作，除了了解什么样的方法是有作用的，研究者能了解什么样的方法是无效的也很重要。

本文的==主要贡献==在于，发现了不恰当的评估准则的使用导致模型的性能被夸大。但作者提出的疑惑并没有完全被解决，我想我们还可以进一步追问以下问题：

- 三元组分数重复程度严重，是否表示评分函数对三元组的区分能力有待加强？
- ReLU 激活之后，许多神经元变为 0，是因为 ReLU 这个函数的不合理，还是因为神经网络结构的不合理？
#
### 第三篇
 ICLR 2020 ｜ You Can Teach An Old Dog New Tricks! On Training Knowledge Graph Embeddings

（作者：Daniel Ruffinelli, Samuel Broscheit, Rainer Gemulla）

#### 类型
这篇文章是一篇实证角度的综述。文章标题的所谓「旧瓶装新酒」，这篇文章的「旧瓶」是什么，「新酒」又是什么呢？

作者认为，不同的嵌入方法包含不同的模型架构、训练策略和超参数优化方法。作者的目标是总结和量化这些维度对模型性能的影响。所谓「旧瓶」是现有的模型架构，「新酒」是作者实验的训练方法，包括损失函数、负样本、正则化方法、优化方法等。作者发现，对已有模型架构使用更先进的技术进行训练，可以表现出更强的性能。

作者总结了知识图谱嵌入模型和训练策略，粗体表示首次使用某种策略的文献。


![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/5.jpg)

regularizer：正则化方法

optimizer：优化方法

reciprocal：互惠

#### 嵌入模型
##### 训练类型
作者根据负样本的生成方法，将训练类型分为三类：

NegSamp：对每个正样本随机打乱三元组中的 subject/relation/object 的位置，可选地验证这样的三元组是否已存在于图中。

1vsAll：忽略采样，将所有打乱（即使存在于图中） subject 和 object 位置的三元组作为负样本，比 NegSamp 代价更高，但如果实体数量有限是可以接受的。

KvsAll：从非空的行 (i, k, *) 或 (*, k, j)，而不是单个三元组，构建 batch。存在于训练数据中的标为正样本，否则为负样本。

##### 损失函数
现有的研究在损失函数上的使用：

- 三元组分数与标签（正或负）之间的均方差
- TransE 使用的 hinge loss
- binary cross entropy [9]
- cross entropy [10]

##### 对等关系
对等关系[11-12]的思想是对头实体和尾实体的预测分数，分别使用两个不同的评分函数。两个评分函数共享实体嵌入，但不共享关系嵌入，因而每个关系拥有两个嵌入。

##### 正则化
- L2 正则化是嵌入模型中最常见的正则化方法，可能是非加权的，或按实体/关系频率进行加权的归一化。
- Lacroix 等人提出了 L3 正则化。
- TransE 在每次更新后将嵌入归一化为 unit norm.
- ConvE 在隐藏层中使用了 dropout。
除现有研究的方法之外，作者还考虑了 L1 正则化，和对实体/关系嵌入使用 dropout。

##### 超参数
现有工作中的超参数有很多种类，例如学习率、batch size、负样本数量、正则化参数等。

#### 实验
对于较老的模型，在尝试不同的训练方式之后，取得了较大幅度的提升（Ours 与 First 的对比），几乎能与最近的工作（Recent）相媲美。表明工作的进步不一定都来自于模型结构的改变，训练方式在性能提升中发挥了重要作用。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/6.jpg)

经过准随机的超参数搜索和贝叶斯优化后，验证集表现最好的模型的超参数如表所示。括号内是不使用该超参数值的最佳配置的 MRR 减少量。基本可以发现，不存在通用的最优训练方式，不同模型所需的训练方式不同。训练方式的改变对模型表现均有一定影响。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/7.jpg)

**Mean Reciprocal Rank (MRR)：**==还有补充==

MRR计算公式如下：
![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/MRR.jpg)

其中|Q|是用户的个数，ranki是对于第i个用户，推荐列表中第一个在ground-truth结果中的item所在的排列位置。

举个例子，有三个用户，推荐列表中正例的最小rank值分别为3，2，1，那么MRR=(1 + 0.5 + 0.33) / 3 = 0.61

取倒数相加


#### 结论

从这篇文章可以见到，关于嵌入的文章创新方向不同，但某种新的训练方法可能只在新的模型上尝试，而不是对大量的其他模型进行尝试。有时我们需要重新审视，一篇新文章的新模型，其关键优势到底是什么？能为其他工作带来多少借鉴。

作者发现，当训练方法合适时，不同模型结构之间的相对性能差异往往会缩小，有时甚至出现反转。这说明，训练策略对模型表现可能有很大的影响。
#

### 3 篇文章小结

知识图谱嵌入这一话题，延续之前的热度，仍然在今年的顶级会议/期刊上出现了优秀的成果。关于第 1 篇文章，再次验证了知识图谱与自然语言处理之间紧密的关系。知识图谱三元组，通过自然语言表述，在利用一些序列模型时，能在三元组分类上取得较好效果。
第 2 篇与第 3 篇文章，从实证研究的角度审视现有工作，表明即使过去数年知识图谱嵌入模型涌现了许多优秀的工作，但实际上在==三元组评分、训练方式==等方面的研究可能并不严谨，为我们今后评判知识图谱嵌入的工作提供了一些新的角度，并提出了研究者应当弥补的问题。

#

### 三元组评分


#
### 训练方式


#

表示学习的过程，其实就是实体链接的过程，首先把实体以及关系随机初始化为一定长度的向量，然后基于训练数据，希望通过类似于翻译的方式得到一个更优的向量表示，使 头实体+关系 尽可能的与尾实体的向量表示相近。在完成训练之后，如何测试、如何衡量我们得到的结果的好坏呢。

下面就是hit@10、mean rank两个指标显身手的时候了。

在测试过程中，对于一对关系及实体，我们将头实体或尾实体替换成任意一种其他的实体（共n-1个，保持另一个实体以及关系不变，只变其中一个实体），这样我们得到了（n-1）个新的关系三元组，然后我们对这些三元组计算实体关系距离，将这n-1个三元组按照距离从小到大排列。

接下来就是计算hit@10、mean rank了，在这个排好序的n-1元素中，我们从第一个开始遍历，看从第一个到第十个是否能够遇到真实的实体，如果遇到了就将（hit@10 +1），这就是hit@10的意义，表示我们的算法能够正确表示三元组关系的能力（在hit@10里  不要求第一个才是对的，能做到前十的能力就可以了）

而对于mean rank是计算在测试集里，平均到第多少个才能匹配到正确的结果。


```

rank_head_sorted = sorted(rank_head_dict.items(),key = operator.itemgetter(1))

rank_tail_sorted = sorted(rank_tail_dict.items(),key = operator.itemgetter(1))



#rank_sum and hits

for i in range(len(rank_head_sorted)):

    if triple[0] == rank_head_sorted[i][0][0]:

        if i<10:

            hits += 1 #如果不到十次就命中了

        rank_sum = rank_sum + i + 1 #统计总命中次数，mean rank就是对这个总数计算平均值

        break



for i in range(len(rank_tail_sorted)):

    if triple[1] == rank_tail_sorted[i][0][1]:

        if i<10:

            hits += 1

        rank_sum = rank_sum + i + 1

        break

step += 1

if step % 5000 == 0:

    print("step ", step, " ,hits ",hits," ,rank_sum ",rank_sum)

    print()



self.hits10 = hits / (2*len(self.test_triple))  #这个是计算的前十个命中的概率

self.mean_rank = rank_sum / (2*len(self.test_triple)) #这个是计算平均第几个命中

```


#
### 第四篇

**TransE：Translating embeddings for modeling multi-relational data。**

期刊/年份：NIPS/2013

作者：A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko

单位：CNRS, Google inc.

关键词：Embedding entities and relationships, Multi-relational data, link prediction



#### 背景
表示学习是深度学习的基石，正式表示学习才能让深度学习可以自由的挖掘更深层次的特征。自word embedding（词嵌入表示）的提出，一种对结构化信息的三元组表示学习也进入研究视野。TransE模型正是一种基于深度学习的知识表示方法，也是Trans系列的开山之作，虽然至今已有7年，但仍然是学习知识表示的重要切入点

#### 问题
如何建立简单且易拓展的模型把知识库中的实体和关系映射到低维向量空间中，从而计算出隐含的关系？

#### 目标
提出一个容易训练的简易模型，包括降低参数数量，且可以适用大规模数据集。由此我们提出TransE方法，将关系建模在低维度的实体表征空间上的做法视为一种翻译操作。


#### 模型：

传统训练知识库中三元组(head,relation,tail)建模的方法参数特别多，导致模型太复杂难以解释，并且需要很大的计算代价，很容易出现过拟合或欠拟合问题。而简单的模型在表现上与复杂的模型几乎一样，但更易拓展。

**思想**：

这是Trans系列的第一篇文章，模型的基本想法是前件的向量表示h与关系的向量表示r之和与后件的向量表示t越接近越好

```
即h+r≈t
```

这里的“接近”可以使用L1或L2范数进行衡量。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/15.jpg)

优化的目标函数使用包含negative sampling的max margin损失函数，即
```
L(y,y’) = max(0,margin−y+y’)
```
，其中y表示正样本的得分，y’表示负样本的得分。最小化这个损失函数可以使正样本的得分越来越高而负样本的得分越来越低，但是两个得分差距大到一定程度（margin）就足够了，再大的话得到的loss也只是0。在这里因为正负样本得分用的是距离，所以要加负号，即最终的损失函数为

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/16.jpg)

其中

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/17.jpg)

为L1或者L2范数。论文中获得负样本并不是随机选取其他的三元组，其中还有一个trick，将正样本三元组中的前件或后件替换为一个随机的实体以此获取负样本。

==**损失函数**：（未看）==

Hinge Loss是一种目标函数（或者说损失函数）的名称，有的时候又叫做max-margin objective。其最著名的应用是作为SVM的目标函数。
其二分类情况下，公式如下： 
l(y)=max(0,1−t⋅y)

其中，y是预测值（-1到1之间），t为目标值（±1）。
 
其含义为，y的值在-1到1之间就可以了，并不鼓励|y|>1，即并不鼓励分类器过度自信，让某个可以正确分类的样本距离分割线的距离超过1并不会有任何奖励。从而使得分类器可以更专注整体的分类误差。



**缺陷**：
但是模型的简单也就带来了问题，它只适合处理一对一的关系，不适合一对多/多对一的关系。举个例子，有两个三元组（中国科学院大学，地点，北京）和（颐和园，地点，北京），使用TransE进行表示的话会得到中国科学院大学的表示向量和颐和园的表示向量很接近，甚至完全相同。但是它们的亲密度实际上可能没有这么大。

**TransE的训练过程如下图**：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/TransE.png)

TransE模型的训练中，第12步是损失函数，对E和L做uniform初始化之后，让正确的h+l-t结果趋近于0，让错误的h‘+l-t’的结果变大，损失函数结果大于0取原值，小于0则取0，这种hinge loss function可以尽可能的将对和错分开，模型使用SGD训练，每次更新可以只更新这个batch里的三元组的向量，因为参数之间并没有冲突。

### 实验及分析
实验在两个数据集上分别完成，分别为WordNet和FreeBase。数据集情况如图所示：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/10.png)

评价指标：测试集中的头实体被移除，轮流换为字典中的任意一个实体，因此首先计算错误三元组的得分并升序保存得分；其次保存正确的实体得分。整个过程重复，同时去掉尾实体而不是头实体。我们报告预测排名和hits@10的平均值，即排名前10位的正确实体的占比，随后移除所有的错误元组，不管是出现在训练集和验证集或者测试集中的，这保证了所有的错误元组不属于数据集。

（1）链接预测（link precision）
论文中并没有详细说明链接预测的具体操作，实质上链接预测可以分为实体预测和关系预测。
- 实体预测：给定已知的头实体（或尾实体）以及关系类，预测对应的尾实体（或头实体）；
- 关系预测：给定已知的头实体和尾实体，预测对应的关系。
作者首先完成的是实体预测。如下图表示不同数据集上各个模型的预测效果：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/11.png)

#### 结果
该图表示在测试集上，给定已知的实体和关系，预测另一个实体，经过升序排序后取最高的前10个三元组，并计算正确的占比，实验表明TransE效果最好。


**MR（mean rank）****平均倒数排名**：
首先 对于每个 testing triple，以预测tail entity为例，我们将（h,r,t）中的t用知识图谱中的每个实体来代替，然后通过fr（h,t）函数来计算分数，这样我们可以得到一系列的分数，之后按照 升序将这些分数排列。

然后，我们需要知道的是f函数值是越小越好，那么在上个排列中，排的越前越好。

现在重点来了，我们去看每个 testing triple中正确答案也就是真实的t到底能在上述序列中排多少位，比如说t1排100，t2排200，t3排60.......，之后对这些排名求平均，Mean rank就得到了。

**Hit10**：还是按照上述进行f函数值排列，然后去看每个testing triple正确答案是否排在序列的前十，如果在的话就计数+1

最终  排在前十的个数/总个数   就是Hit@10

**MRR**：是一个国际上通用的对搜索算法进行评价的机制，即第一个结果匹配，分数为1，第二个匹配分数为0.5，第n个匹配分数为1/n，如果没有匹配的句子分数为0。最终的分数为所有得分之和。

--

随后作者划分了四种类型的关系，分别为一对一、一对多、多对一、多对多，实验方法与前面相同，只是将测试集划分的更细，模型效果如下：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/12.png)

作者给出预测的样例，即给定头实体和关系类，从词典中预测尾实体：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/13.png)

最后，作者完成了根据少量的样本预测新关系：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/14.png)

其中左图表示测试集中平均排名，右图表示Hits@top10中正确的比例。可以发现当训练集越大，TransE的平均排名下降的最快，而top10占比上升的最快，且对应结果均为最优。

### 代码：


![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/19.jpg)


```
// TransE算法
#实体对应的表示矩阵n*k,n表示实体个数，k为表示向量的维度，也是模型需要优化的参数
self.ent_embeddings = tf.compat.v1.get_variable(name="ent_embedding", shape=[self.entity_total, self.hidden_size],
                                                  initializer=tf.contrib.layers.xavier_initializer(uniform=False))
#关系对应的表示矩阵m*k,m表示关系个数，k为表示向量的维度，也是模型需要优化的参数                                                
self.rel_embeddings = tf.compat.v1.get_variable(name="rel_embedding", shape=[self.relation_total, self.hidden_size],
                                                  initializer=tf.contrib.layers.xavier_initializer(uniform=False))

self.pos_h = tf.compat.v1.placeholder(tf.int32, [None])  #训练或预测输入，正样本H实体对应的ID
self.pos_t = tf.compat.v1.placeholder(tf.int32, [None])  #训练或预测输入，正样本T实体对应的ID
self.pos_r = tf.compat.v1.placeholder(tf.int32, [None])  #训练或预测输入，正样本R关系对应的ID
self.neg_h = tf.compat.v1.placeholder(tf.int32, [None])  #训练或预测输入，负样本H实体对应的ID
self.neg_t = tf.compat.v1.placeholder(tf.int32, [None])  #训练或预测输入，负样本T实体对应的ID
self.neg_r = tf.compat.v1.placeholder(tf.int32, [None])  #训练或预测输入，负样本R关系对应的ID
#根据ID，获取正负样本（h,t,r）对应的表示向量
pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h)   
pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t)
pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r)
neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h)
neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t)
neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r)
#计算样本的三元距离	        
if self.L1_flag: #如果L1范数
	pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keepdims=True)
	neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keepdims=True)
	self.predict = pos
else:  #如果L2范数
	pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keepdims=True)
	neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keepdims=True)
	self.predict = pos
#求损失函数
with tf.name_scope("output"):
	self.loss = tf.reduce_sum(tf.maximum(pos - neg + self.margin, 0))

```



### 简评
作者提出的模型简洁，且计算量小，可以支持链接预测、对大量数据集进行处理。不过也存在一系列问题：模型过于简单也是其弊端，不能够很好的处理更复杂的知识网络，换句话说是不能够有效充分的捕捉实体对间语义关系。

作者另外提出未来可以和文本结合学习，并提出一种使用TransE框架完成关系抽取任务的想法。

本文提出了一种将实体与关系嵌入到低维向量空间中的简单模型，弥补了传统方法训练复杂、不易拓展的缺点。尽管现在还不清楚是否所有的关系种类都可以被本方法建模，但目前这种方法相对于其他方法表现不错。TransE更是作为知识库vector化的基础，衍生出来了很多变体。

### 创新点
1.**构建多元关系数据**：通常关系数据包括单一关系数据（single-relational data）和多元关系数据（multi-relational data）。单一关系通常是结构化的，可以直接进行简单的推理；多元关系则依赖于多种类型的实体和关系，因此需要一种通用的方法能够同时考虑异构关系。

2.**关系可以作为嵌入空间之间的翻译**：我们提出一种基于能量机制的模型来训练低维度实体嵌入。在TransE中，关系类被表征为翻译嵌入式表征。如果实体对存在，则头实体与之对应的关系向量之和和尾实体尽可能相同。

### 资源

数据集 WordNet http://wordnet.princeton.edu/wordnet/download/

数据集 Freebase http://developers.google.com/freebase/

Code: https://github.com/thunlp/KB2E



#

### 第五篇
**TransH：Knowledge Graph Embedding by Translating on Hyperplanes**

作者：Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen

期刊/年份：AAAI/2014

单位：
Sun Yat-sen University

microsoft

关键词：knowledge graph embedding, Multi-relational data



#### 背景
TransE是一种经典的知识表示学习方法，其通过对头实体、尾实体及对应关系进行建模，设计 d(h+l−t) 的能量函数，并运用负采样和随机梯度下降的方法对待训练的向量进行调整，从而能够得到不错的低维向量。TransE模型在链接预测上达到的最优，对知识补充起到了最好的效果。

然而TransE模型结构非常简单，虽然可以在大量数据的条件下可以快速有效的进行训练，但由于模型过于简单，并不能够很好的表征实体对之间的语义关系。另外对于处理复杂关系数据（一对多、多对一和多对多）时，过于简单的假设可能导致错误的表征。例如对于一对多情况，即同一个头实体 h 和关系 l可能对应多个尾实体 h1,h2,... 所以TransE模型训练后的结果是 h1≈h2≈...  这很明显是错误的。因此需要对其进行改进。TransH就是对TransE模型的改进。

#### ==创新点==
1.作者提出TransH模型，其可以构建关系的超平面并执行翻译操作。这样可以很好的保存的关系映射特性，且模型复杂度与TransE相似。

2.作为一种实用的知识图谱，往往是不完整的。在训练过程中如何构建负样本对于降低错误标签是非常重要的。利用一对多或多对一的关系映射属性，我们提出一种简单的策略来价格低错误标签的可能性。

3.实验表明，与TransE相比，TransH在预测精度方面有了显著的改进，并具有可比较的扩展能力。



#### 问题
对知识库中的实体关系建模,特别是一对多,多对一,多对多的关系。设计更好的建立负类的办法用于训练。

#### 模型
**文中符号表示**
![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/15.png)

**关系映射分析**

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/16.png)

**算法模型详解**

wr：法向量

dr：超平面内的翻译向量


假设一个三元组（h，r，t）对应的向量分别是h，r，t，关系r的对应的投影矩阵设为wr，如图所示：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/17.png)

关系 r 对应一个超平面，实体空间中的两个实体 h,t通过关系映射矩阵wr投影到这个超平面上。这种投影机制并非普通的垂直投影，投影的方式则通过矩阵相乘形式，取决于对应的实体，投影后的向量分别为：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/18.png)

训练的损失函数则于TransE一样，采用的负采样方法，最小化正确三元组的得分，最大化错误三元组的得分：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/19.png)

对于损失函数后面的解释：

（1）C表示超参数，是一个权值；

（2）括号内前一个累和表示所有实体累计计算实体向量的长度的平方与1的差的最大值（与0比较），可知该项约束了实体向量长度小于等于1；

（3）后一个累和表示对于所有关系类映射向量正交分解，即保证每个关系类在一定程度上所表示的空间互不相关。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/20.png)

#### 解决方法

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/23.png)



#### 几何含义

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/22.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/23.png)

#### 目标函数

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/25.png)

如下图所示：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/26.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/27.png)





### 实验及分析
作者在三个任务上进行了验证，相比TransE模型更具有通用性和可扩展性。

1）链接预测：这部分与TransE相同，链接预测的概念请参考【TransE】，具体的实验细节和参数设置省略，实验对比如图所示：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/21.png)
==对于WN18-MEAN的结果存在疑惑==


2）三元组分类

这个任务主要是二分类，给定一个三元组（h，r，t），判断这个三元组是否是正确的，即头尾实体。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/28.png)

其中Hits@10表示对所有三元组中，预测结果在前10个中的占比。

3）文本关系抽取：这一部分正是TransE在总结部分的未来展望，TransH将其实现这个任务的对比。关系抽取任务可以通过知识表示来完成，也属于一种知识补充。作者使用基于远程监督的关系抽取，评价指标则为P-R曲线与对应的AUC面积。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/29.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/30.png)

precision：精确率（和准确率不一样）

recall：召回率


EG：

假设一共有10篇文章，里面4篇是你要找的。根据你某个算法，你认为其中有5篇是你要找的，但是实际上在这5篇里面，只有3篇是真正你要找的。


```
那么你的这个算法的precision是3/5=60%，也就是，你找的这5篇，有3篇是真正对的

这个算法的recall是3/4=75%，也就是，一共有用的这4篇里面，你找到了其中三篇。
```



EG：假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本，系统查找出50个，其中只有40个是真正的正样本，计算上述各指标。


```
TP: 将正类预测为正类数 40
FN: 将正类预测为负类数 20
FP: 将负类预测为正类数 10
TN: 将负类预测为负类数 30
```

准确率(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) = 70%

精确率(precision) = TP/(TP+FP) = 80%

召回率(recall) = TP/(TP+FN) = 2/3



上图一为精确度测试，即测试集中目标实体对关系被正确预测的比例；图二的两个图表示P-R曲线，这部分则是多类分类任务。左图表示基于Sm2r和TransE/TransH的两个得分函数共同训练，右图表示仅使用TransE/TransH的得分函数训练，可知TransE和TransH均比现有的模型效果好，TransE和TransH对比下可知TransH优于TransE。



### 资源
数据集 WordNet:http://wordnet.princeton.edu/wordnet/download/

数据集 Freebase:
http://developers.google.com/freebase/

Code:https://github.com/thunlp/KB2E

### 代码

```
// TransH算法

def calc(self, e, n): #计算实体向量向关系超平面的投影，与TransE不同之处
	norm = tf.nn.l2_normalize(n, 1)
	return e - tf.reduce_sum(e * norm, 1, keepdims=True) * norm
#实体向量、关联向量和超平面法向量
self.ent_embeddings = tf.compat.v1.get_variable(name="ent_embedding", shape=[self.entity_total, self.hidden_size],
                                                  initializer=tf.contrib.layers.xavier_initializer(uniform=False))
self.rel_embeddings = tf.compat.v1.get_variable(name="rel_embedding", shape=[self.relation_total, self.hidden_size],
                                                  initializer=tf.contrib.layers.xavier_initializer(uniform=False))
self.normal_vector = tf.compat.v1.get_variable(name="normal_vector", shape=[self.relation_total, self.hidden_size],
                                                 initializer=tf.contrib.layers.xavier_initializer(uniform=False))#
#提取正负样本向量
pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h)
pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t)
pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r)     
neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h)
neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t)
neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r)   
#提取投影法向量   
pos_norm = tf.nn.embedding_lookup(self.normal_vector, self.pos_r)
neg_norm = tf.nn.embedding_lookup(self.normal_vector, self.neg_r)
#对实体进行投影
pos_h_e = self.calc(pos_h_e, pos_norm)
pos_t_e = self.calc(pos_t_e, pos_norm)
neg_h_e = self.calc(neg_h_e, neg_norm)
neg_t_e = self.calc(neg_t_e, neg_norm)
#计算实体投影向量和关系向量三元组的距离
if config.L1_flag:
	pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keepdims=True)
	neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keepdims=True)
	self.predict = pos
else:
	pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keepdims=True)
	neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keepdims=True)
	self.predict = pos
#计算损失函数
with tf.name_scope("output"):
	self.loss = tf.reduce_sum(tf.maximum(pos - neg + self.margin, 0))         

```


### 总结
这个模型TransE主要解决了TransE不能够很有效的进行复杂关系的建模，并提出一种关系映射方法解决了这个问题，在包括链接预测、三元组分类和关系抽取任务上表现最好，另外作者改进了负采样的策略，提出一种概率采样的办法，降低错误标签带来的问题。

当然TransH也存在一定的问题：头尾实体依然处于相同的语义空间，而每个三元组的关系可能关注头尾实体的不同属性，即每个关系对应的头尾实体应在不同的语义空间中表征，因此TransR模型由此诞生。

### 简评
论文提出的TransH模型，为了解决TransE对一对多，多对一，多对多关系建模的难题。它权衡模型复杂度和模型表达能力。而且还设计了复杂取样的办法用于训练。

#

### 第六篇
TransR：**Learning Entity and Relation Embeddings for Knowledge Graph Completion**

年份/期刊：2015/AAAI

作者：Yankai Lin
, Zhiyuan Liu
, Maosong Sun,
, Yang Liu
, Xuan Zhu




#### 背景
TransH在TransE基础上做出的改进，提高了知识表示的效果，在一定程度上解决了复杂关系的处理，同时在链接预测、三元组分类和关系抽取任务上相比传统的方法（距离模型、非结构模型、单层神经网络、双线性模型等）达到最优，然而TransH也存在一定的问题。TransR作者发现TransH模型虽然有效的处理了复杂语义关系表示，但两个实体仍然处于相同的语义空间，因此不能够充分表示实体与关系的语义联系。

#### 摘要和引言
知识图谱的完成可以实现目标实体之间的链接预测。本文我们研究了知识图谱表征的方法。最近TransE和TransH模型通过将关系视为一种从头实体到尾实体的翻译机制来获得实体和关系的表征。事实上一个实体可能有多个不同方面（特征），关系可能关注实体不同方面的特征，公共的实体特征空间不足以表征。本文，我们提出TransR模型构建实体和关系表征，将实体空间和关系空间相分离。然后我们以这种方式训练表征向量：首先通过将实体映射到关系空间中，其次在两个投影实体之间构建翻译关系。实验中，我们在三个任务上完成了验证，分别是链接预测、三元组分类和关系抽取。实验效果表明相比之前的基线模型，包括TransE和TransR，得到了一定的提升。

完善的知识图谱旨在预测给定两个实体对的关系，即链接预测，期现如今面临的挑战包括（1）图谱中的结点包含不同类型和属性的实体、（2）边表示不同类型的关系。对于知识补全，我们不仅仅只是判断实体对是否存在关系，也需要预测具体的关系类。基于此，传统的链接预测方法则无法实现链接预测。最近一种新提出的表示学习是指将实体和关系嵌入到连续的向量空间中。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/32.png)

TransR的主要思路如图所示：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/33.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/34.png)


#### 问题
一个实体是多种属性的综合体，不同关系关注实体的不同属性。直觉上一些相似的实体在实体空间中应该彼此靠近，但是同样地，在一些特定的不同的方面在对应的关系空间中应该彼此远离。为了解决这个问题，我们提出了一种新的方法，将实体和关系投影到不同的空间中，也就是实体空间和多元关系空间（也即是特定关系的实体空间），在对应的关系空间上实现翻译。

在前面的TransE和TransH中，都假设的是关系向量和实体向量在同一个向量空间中，TransR将每个关系都对应一个不同的关系空间和一个关系向量，在对实体进行计算时，先将实体向量投影到关系空间，然后进行比较。

#### 模型（方法）
TransR的基本思想如图1所示。对于每个元组（h，r，t），首先将实体空间中的实体通过Mr向关系r投影得到hr和tr，然后使hr+r≈tr。特定的关系投影（彩色的圆圈表示）能够使得头/尾实体在这个关系下真实的靠近彼此，使得不具有此关系（彩色的三角形表示）的实体彼此远离。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/31.png)

此外，在一个特定的关系下，头-尾实体对通常展示出不同的模型。仅仅通过单个的关系向量还不足以建立实现从头实体到尾实体的所有翻译。例如，具有关系“location location contains”头-尾实体有很多模式，如country-city，country-university，continent-country等等。沿着分段线性回归（Ritzema and others 1994）的思想，通过对不同的头-尾实体对聚类分组和学习每组的关系向量，我们进一步提出了基于聚类的TransR（CTransR）。

我们通过WordNet和Freebase上的链接预测，元组分类和关系事实抽取来评估我们的模型。实验结果表明与最先进的模型相比，我们的模型取得了显著的提高。


![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/35.png)

缺点：

对于每一个关系，关系矩阵都是一定的。对于每一个关系都要学习一个矩阵，需要的参数量太大。

#### 实验结果
实验主要包括三个任务，实验的具体细节与TransH相同 ，不做详细介绍。

（1）链接预测：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/36.png)

unif：全部一起

bern：按照抽样公司抽样

（2）三元组分类：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/37.png)



（3）关系抽取：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/38.png)

#### 代码

```
// TransR算法
#实体表示矩阵
self.ent_embeddings = tf.compat.v1.get_variable(name="ent_embedding", shape=[self.entity_total, self.sizeE],
                                                      initializer=tf.contrib.layers.xavier_initializer(uniform=False))#
#关系表示矩阵
self.rel_embeddings = tf.compat.v1.get_variable(name="rel_embedding", shape=[self.relation_total, self.sizeR],
                                                      initializer=tf.contrib.layers.xavier_initializer(uniform=False))
#实体表示空间向关系表示空间的变换矩阵，并初始化为单位矩阵
rel_matrix = np.zeros([self.relation_total, self.sizeR * self.sizeE], dtype=np.float32)
for i in range(self.relation_total):
	for j in range(self.sizeR):
		for k in range(self.sizeE):
			if j == k:
				rel_matrix[i][j * self.sizeE + k] = 1.0
self.rel_matrix = tf.Variable(rel_matrix, name="rel_matrix")

#提取实体，并继续维度变换，感觉除了变换矩阵外，不需要维度变换
pos_h_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h), [-1, self.sizeE, 1])
pos_t_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t), [-1, self.sizeE, 1])
pos_r_e = tf.reshape(tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r), [-1, self.sizeR])
neg_h_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h), [-1, self.sizeE, 1])
neg_t_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t), [-1, self.sizeE, 1])
neg_r_e = tf.reshape(tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r), [-1, self.sizeR])
pos_matrix = tf.reshape(tf.nn.embedding_lookup(self.rel_matrix, self.pos_r), [-1, self.sizeR, self.sizeE])
neg_matrix = tf.reshape(tf.nn.embedding_lookup(self.rel_matrix, self.neg_r), [-1, self.sizeR, self.sizeE])

#实体表示向量变换为关系空间内的向量
pos_h_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(pos_matrix, pos_h_e), [-1, self.sizeR]), 1)
pos_t_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(pos_matrix, pos_t_e), [-1, self.sizeR]), 1)
neg_h_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(neg_matrix, neg_h_e), [-1, self.sizeR]), 1)
neg_t_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(neg_matrix, neg_t_e), [-1, self.sizeR]), 1)

#计算三元组距离
if config.L1_flag:
	pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keepdims=True)
	neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keepdims=True)
	self.predict = pos
else:
	pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keepdims=True)
	neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keepdims=True)
	self.predict = pos
#计算损失函数	
with tf.name_scope("output"):
	self.loss = tf.reduce_sum(tf.maximum(pos - neg + self.margin, 0))

;

```


#### 总结和评价
本文作者提出的TransR和CTransR模型可以将相同关系的三元组映射到对应关系的空间中，有效的对三元组进行语义表示，在包括链接预测、三元组分类和关系抽取任务上均实现最好效果。作者提出三个未来工作，包括


```
（1）利用推理信息增强图谱的表征
（2）探究文本与图谱的表示模型
（3）基于CTransR，研究更复杂的模型
```
![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/39.png)

TransR还有一些缺点。例如引入的空间投影策略增加了计算量、头尾实体一同投影到关系空间，而未考虑到头尾实体的不同语义类型、仅将实体投影到关系空间中还不够完全提高语义表能力等。TransD模型试图改进这些不足之处。
#


### 第7篇
TransD: **knowledge graph embedding via dynamic mapping matrix**

作者：Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu and Jun Zhao

期刊/年份：ACL2015

单位：中国科学院自动化研究所 National Laboratory of Pattern Recognition (NLPR)

关键词：knowledge graph embedding, link prediction.



#### 背景
知识图谱作为人工智能应用的重要资源，表示学习对知识图谱的完善和应用至关重要。先前提出的TransE、TransH、TransR模型对表示学习提升不少，表示学习对关系抽取、三元组分类以及链接预测等方面具有作用。TransD模型改进TransR，认为不同的实体应映射到不同的语义空间中，且减少了计算量。

#### 摘要和引言
知识图谱对于大量的人工智能应用来说是非常有用的资源，但是其距离完善还有一段距离。先前的工作例如TransE、TransH和TransR，认为头实体到尾实体可以被认为是一种翻译，且CTransR获得最优效果。本文，我们提出一个细粒度模型，叫TransD，且相比之前的模型有所提高。在TransD中，我们使用两个向量来表征两个实体（头实体和尾实体）。首先第一个向量表征实体关系，另一个被用来构建动态映射矩阵。相比TransR/CTransR模型，TransD不仅考虑到关系的多样性，也考虑到实体的多样性。TransD有较少的参数，且TransD参数较少，没有矩阵向量乘法运算，可以应用于大型图数据。实验中，我们在两个标准任务上评估了我们的模型。评估的结果表明我们的方法比最优模型更好。


像WordNet、FreeBase、YaGo一样的知识图谱在许多AI应用，例如关系抽取、问答等。这些通常包含大量的结构化数据，形如（h，r，t）。

TransR模型包含如下几个缺点：

（1）对于特定的关系r，所有实体共享同一个语义空间Mr。因此实体需要映射到不同的语义空间中；

（2）实体和关系的投影操作是一个连续迭代的操作，仅依靠关系进行推理是不足的；

（3）矩阵向量带来大量的参数运算量。

#### 方法

本文提出一种全新的方法TransD来为图谱进行建模。如图所示：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/41.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/42.png)

**算法要点：**

每一个实体和关系向量都使用两个向量进行表示，第一个向量表示实体或关系的语义向量，另外一个向量用于得到将实体向量映射到关系向量空间的映射矩阵。

由此，每一个实体-关系对都有唯一的映射矩阵，而且不像TransR需要进行矩阵和向量的乘积运算，这个运算只需要进行向量与向量的乘积运算，计算量大大降低。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/47.png)


#### 实验和分析

数据集包括四个，分别如图所示：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/43.png)

实验包括两个部分：

（1）三元组分类：任务给定三元组，判定当前三元组是否正确。实验结果如图所示

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/44.png)


![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/45.png)

（2）链接预测：给定实体和关系，预测另一个实体。实验结果如图所示

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/46.png)

#### 代码

```
// TransD算法

def calc(self, h, t, r):  %计算实体向量表示向关系空间的转换
        return tf.nn.l2_normalize(h + tf.reduce_sum(h * t, 1, keepdims=True) * r, 1)
#实体表示向量
self.ent_embeddings = tf.compat.v1.get_variable(name="ent_embedding", shape=[self.entity_total, self.hidden_size],
                                                  initializer=tf.contrib.layers.xavier_initializer(uniform=False))
#关系表示向量
self.rel_embeddings = tf.compat.v1.get_variable(name="rel_embedding", shape=[self.relation_total, self.hidden_size],
                                                  initializer=tf.contrib.layers.xavier_initializer(uniform=False))
#实体转换向量
self.ent_transfer = tf.compat.v1.get_variable(name="ent_transfer", shape=[self.entity_total, self.hidden_size],
                                                initializer=tf.contrib.layers.xavier_initializer(uniform=False))
#关系转换向量
self.rel_transfer = tf.compat.v1.get_variable(name="rel_transfer", shape=[self.relation_total, self.hidden_size],
                                            initializer=tf.contrib.layers.xavier_initializer(uniform=False))
#提取查找向量
pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h)
pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t)
pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r)
pos_h_t = tf.nn.embedding_lookup(self.ent_transfer, self.pos_h)
pos_t_t = tf.nn.embedding_lookup(self.ent_transfer, self.pos_t)
pos_r_t = tf.nn.embedding_lookup(self.rel_transfer, self.pos_r)

neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h)
neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t)
neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r)
neg_h_t = tf.nn.embedding_lookup(self.ent_transfer, self.neg_h)
neg_t_t = tf.nn.embedding_lookup(self.ent_transfer, self.neg_t)
neg_r_t = tf.nn.embedding_lookup(self.rel_transfer, self.neg_r)

#进行空间转换
pos_h_e = self.calc(pos_h_e, pos_h_t, pos_r_t)
pos_t_e = self.calc(pos_t_e, pos_t_t, pos_r_t)
neg_h_e = self.calc(neg_h_e, neg_h_t, neg_r_t)
neg_t_e = self.calc(neg_t_e, neg_t_t, neg_r_t)
#计算元组距离
if config.L1_flag:
	pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keepdims=True)
	neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keepdims=True)
	self.predict = pos
else:
	pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keepdims=True)
	neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keepdims=True)
	self.predict = pos
#计算损失函数
with tf.name_scope("output"):
	self.loss = tf.reduce_sum(tf.maximum(pos - neg + self.margin, 0))
;
```
#### 简评
如果TransD的所有投影向量为0，TransD就是TransE。类似的还有TransR/CTransR，他们对每个relation定义了一个mapping矩阵，参数更多计算复杂度更大。


#
### 第八篇
TransA: **An Adaptive Approach for Knowledge Graph Embedding**

作者：HanXiao,MinlieHuang,HaoYu,XiaoyanZhu

期刊/年份：CL/2015


#### 背景
先前的知识表示方法（TransE、TransH、TransR、TransD、TranSparse等）的损失函数仅单纯的考虑h+r和t在某个语义空间的欧式距离，认为只要欧式距离最小，就认为h和t的关系为r。显然这种度量标准过于简单，虽然先前的工作在得分函数上做出了不错的改进，但训练的损失函数制约了表示能力，因此，本文TransA模型的提出，主要对损失函数进行改进。

虽然TransA的提出是在TransD、TranSparse之前，但实践表明TransA的提出很有价值。

#### 摘要和引言

知识表示在人工智能领域内是非常重要的任务，许多研究试图将知识库中的实体和关系表示为一个连续的向量。通过这些尝试，基于翻译模型的表示方法是通过最小化头实体到尾实体的损失函数。尽管这些策略非常成功，但其损失函数过于简单，不能够很好的表示复杂多变的知识图谱。为了解决这些问题，我们提出TransA，一种对表示向量的自适应度量方法。根据度量学习的想法提出一个更灵活的嵌入方法。实验在几个基线数据集上完成，我们的模型获得了最优效果。

最近研究均涉及到知识图谱，像问答系统等需要对图谱进行表示，现如今提出的方法有TransE、TransH等。然而这些方法的度量标准仅仅是实体之间的欧氏距离，过于简单的损失函数不能够处理复杂多变的图谱。

（1）由于缺乏灵活的损失函数，当前的翻译模型均是应用球形等位超平面，因此越靠近中心，实体对与对应关系的向量越相似。如图所示，这是TransE模型在FreeBase上训练的向量通过PCA降维得到的图：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/48.png)

橘黄色的为头实体，橘黄色的线与箭头则为对应的关系向量。蓝色的叉表示正确的尾实体，红色的圆点则是错误的尾实体，可知当简单的使用欧式距离来评判，会掺和进大量错误的实体。由于图谱是复杂多变的，这一点很难避免。

（2）另外，由于过于简单的损失函数，使得当前几种翻译模型在对向量的每一个维度的训练处理方法相同。如图所示

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/49.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/50.png)

#### 相关工作和主要贡献

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/51.png)

#### 方法

##### 自适应度量分值函数

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/55.png)

##### 椭球面

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/56.png)

##### 特征加权

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/57.png)

##### 实现细节

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/58.png)

### 结果

1. 链路预测

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/59.png)

2. 三元组分类

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/60.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/61.png)


### 简评
感觉这篇文章的思路比较简单，就是针对当前模型的一些不足，更换了一个损失度量函数。但是几点还是值得学习的，首先通过图像来描述不同的损失度量函数，给人一个更直观的感觉；其次针对向量表示中的区别对待，感觉很有attention mechanism的感觉，对不同的triple关注向量表示的不同维度，以取得最好的效果，这点是非常值得借鉴参考的。


#

### 第九篇

TranSparse：**Knowledge Graph Completion with Adaptive Sparse Transfer Matrix**

作者：Guoliang Ji;Kang Liu;Shizhu He;Jun Zhao

期刊/年份：AAAI/2016

#### 背景

先前的基于深度学习的知识表示模型TransE、TransH、TransR（CTransR）和TransD模型均一步步的改进了知识表示的方法，在完善知识图谱补全工作上逐渐提高效果。通过先前的模型，我们基本掌握了知识表示的学习方法：首先通过投影策略将实体和关系映射到对应的语义空间，其次均使用得分函数 f（h，t）= ||h + r - t||表示实体对的评分。另外使用负采样生成错误样本进行训练，使得正确的样本得分函数值降低，错误样本的得分函数值升高。然而这些模型均忽略了图谱的两个重要特性：**异质性（heterogeneity）和不平衡性（imbalance）**。图谱中的异质性是指不同关系对应的实体对数量不一致，例如对于关系r链接的所有实体对数量可能非常多，而对于r'链接的所有实体对数量可能只有1个。不平衡性是指头尾实体的数量不一致，例如形如对于（地名，local，洲名）的三元组，地名可能成千上万个，而洲名只有七个。由于数量的不对等，可知数量较多的对应关系的实体对或头尾实体，它们所包含的信息应该越多，而前面的几种模型忽略了这一点，使得针对每个实体对都用同样的方法训练，**势必会导致数量多的部分出现欠拟合，数量少的部分出现过拟合**。因此本文TranSparse试图改进这一点。

#### 摘要和引言

我们构建完整的知识图谱通过对每个实体和关系进行表征到连续空间中。所有先前的工作包括TransE、TransH、TransR（CTransR）和TransD忽略了异质性（一些关系链接了大量的实体对，而另一些则链接少数的实体对）和非平衡性（相同关系的所有实体对中，头尾实体的数量不相同）。本文我们提出一个新的方法叫TranSparse来解决这两个问题。在TranSparse中，变换矩阵被替换为动态稀疏矩阵，稀疏因子由指定关系的实体或实体对的数量所决定。实验中，我们设计了结构化和非结构化稀疏模式的变换矩阵，并分析他们的优劣性。我们在三元组分类和链接预测两个任务上评估了我们的方法，实验结果表明TranSparse比之前的方法效果好。

知识图谱是由结点和结点之间相连的边组成，我们定义图谱中结点与边组成的元素为三元组，定义为（h，r，t）分别为头实体、关系和尾实体。虽然当前的图谱包含大量的三元组，但仍然是完整的。最近知识表示通过将实体和关系嵌入到连续的空间中来表示图谱成为主流的方法。现有的两种策略分别是基于张量分解（例如RESCAL）和神经网络模型（例如TransE/H/R/D）。然而这些方法均忽略了图谱的异质性和不平衡性。异质性可能导致过拟合或欠拟合，不平衡性可能导致训练的不平等。

**本文处理这种问题的策略是引用稀疏矩阵**。首先对于异质性，提出TranSparse（Share），稀疏因子取决于关系链接对应的实体对数量，且两个实体对应的关系投影矩阵是相同的。对于不平衡性，提出TranSparse（Separate），每个关系对应的实体对中，头尾实体使用不同的关系投影矩阵。

#### 创新点（贡献）

各个算法的复杂度：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/62.png)


本文的主要贡献：
- 提出一种新的模型考虑异质性和不平衡性；
- 方法是有效的，且含有少量的参数；
- 提出了两种稀疏模式，并分析优劣；
- 在三元组分类和链接预测任务上达到了最优效果

### 模型
#### 稀疏矩阵

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/63.png)

#### 为什么选择稀疏矩阵，而不是低秩矩阵？

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/64.png)

0的个数↑  非0个数↓ 自由度↓

稀疏矩阵：0的个数很多，让0元素保持不变，那么可变的元素就变少了，自由度就变低了，自由度越低，包含的有效信息越少，对解决异质性和非平衡性很有效。

#### TranSparse模型

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/65.png)



#### 训练目标

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/66.png)

#### 实现细节

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/67.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/68.png)



### 实验以及分析

本节，实验主要包括三元组分类和链接预测。数据集为WordNet的子集WN11和WN18，FreeBase的子集FB15k和FB13，实验数据集的统计信息如图所以：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/70.png)

1. 三元组分类：三元组旨在对给定三元组的正确与否进行分类。实验表明我们的方法达到最优。分析：
- 相比先前的工作，我们的模型能够很好的处理异质性和不平衡性。
- Separate比Share效果好；
- 非结构化比结构化效果好。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/71.png)



2. 链接预测：链接预测旨在预测未知的头实体或尾实体，评价标准则为 Hist@10Hist@10Hist@10 。实验表明：
- 模型效果达到最优；
- 对于一对多、多对一和多对多的关系模式依然有效；
- 非结构化稀疏矩阵效果最好

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/72.png)

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/73.png)

### 总结
本文我们提出一种TranSparse模型用于补全知识图谱，考虑先前工作忽略了异质性和不平衡性，我们引入了稀疏变换矩阵。实验表明我们的模型能够达到最优。在未来工作，我们将探究最优的稀疏模式用于投影矩阵。

#

### 第十篇
TransG：**A Generative Mixture Model for Knowledge Graph Embedding**

期刊/年份:arxiv/2015

作者：Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu

单位：清华大学 State Key Lab on Intelligent Technology and Systems

关键词：knowledge graph embedding, generative mixture model, multiple relration semantics.

#### 问题
解决多关系语义(multiple relation semantics)的问题。

#### 创新点

1.提出了一个新模型TransG，解决多关系问题。

2.多关系语义分析可以提高三元组的分类准确度。

#### 模型
传统的基于翻译的模型采用h_r + r = t_r(其中，h_r为头部实体，t_r为尾部实体，r为头部
实体跟尾部实体的关系)，仅仅对一个关系赋予一种翻译向量。它们不能细分多关系语义，比如：(Atlantics, HasPart, NewYorkBay)和(Table, HasPart, Leg)两个的关系都是HasPart，但是这两个的关系在语义上不同，第一个是“部件”的关系，第二个是“位置”的关系。TransG能够解决关系的多语义问题。如图所示，多关系语义分析可以提高三元组的分类准确度。

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/74.png)

#### 数据集
数据集 WordNet http://wordnet.princeton.edu/wordnet/download/

数据集 Freebase http://developers.google.com/freebase/

#### 相关工作
大多数都已介绍，这里就只说明CTransR，其中关系的实体对被分类到不同的组，同一组的实体对共享一个关系向量。相比较而言，TransG不需要对聚类的预处理。

#### 简评
这篇文章的idea比较重要，考虑到一种关系存在的多语义问题，相当于对关系进行了细化，就是找到关系的隐形含义，最终从细化的结果中选出一个最佳的关系语义。这个在应用中很有意义，不同的语义可能需要不同的应对方法，可以借鉴。


### 第十一篇
TransM：**Transition-based Knowledge Graph Embedding with Relational Mapping Properties**

作者：MiaoFan,QiangZhou,EmilyChang,ThomasFangZheng

期刊/年份：PACLIC/2014



#### 背景
TransM算法是基于TransE算法进行的改进，目的还是为了解决在TransE中无法处理的一对多、多对一、多对多和自反关系问题。在TransM算法中将这个问题描述成了TransE在处理多类型关系时不够灵活和不能很好的得到表征结果，我觉得这个解释并没有在TransH解释的足够清楚。


#### 创新点

TransM的思路还是和TransE是一致的，就是关系是实体之间的翻译功能，使用的损失函数还是TransE是一样的，还是使得正例的势能差值最大，负例的势能差值最小。为了解决多类型关系这个问题，TransM认为不同的关系类型应该具有不同的权重，利用不同的权重来更新参数。

具体意思就是一对一关系类型的三元组在计算势能差值时的权重最高，因为他的权重完全是由这个一个三元组对儿来表达，但是多对一的关系的权重较低，因为多对一的权重应该由这多个三元组对儿一起来表达，所以在每个三元组对儿身上的权重就应该下降。

利用给不同的关系的三元组对儿不同的权重，来调整不同的三元组对儿的势能差值在对合页损失函数的影响。


#### 模型

TransM的做法和TransE相比，就是多了个权重系数

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/75.png)

其中权重系数的计算方法是：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/76.png)

损失函数和TransE的一样：

![image](C:/Users/baiye/Desktop/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/77.png)

